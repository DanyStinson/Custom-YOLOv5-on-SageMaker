{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc39d14-7603-4588-8f35-e8c29320b90c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Custom YOLOv5 Train and Deploy locally on Amazon SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a9b33-2052-4c57-aa4f-b4b26e1c1d62",
   "metadata": {},
   "source": [
    "In this notebook we will train a custom YOLOv5 object detection CV model within Amazon SageMaker Studio. \n",
    "\n",
    "**Steps:**\n",
    "\n",
    "0. Initial configuration.\n",
    "1. Create a labeling job in Amazon SageMaker GroundTruth.\n",
    "2. Download images and labels from the labeling job.\n",
    "3. Train the custom YOLOv5 model.\n",
    "4. Make inferences with the created model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f3d22-a857-40b5-8cb5-3185dd353233",
   "metadata": {},
   "source": [
    "## 0. Initial Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b786198-5500-4fbe-bc2b-b36bbf109125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/ultralytics/yolov5\n",
    "!pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22293fe-b5c2-42c0-906a-add1fbf406e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy\n",
    "import torch \n",
    "import os\n",
    "import boto3\n",
    "from sklearn.model_selection import train_test_split\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579568ba-54f4-4441-9c48-7417d9b637da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"training_data/images/train\", \"training_data/labels/train\",\n",
    "            \"training_data/images/validation\", \"training_data/labels/validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0490bf13-b2f8-44cd-b270-4236f8a22b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in dirs:\n",
    "    !mkdir -p {directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd1da6-e457-4e2b-acf3-d085643aa8c3",
   "metadata": {},
   "source": [
    "## 1. Create a labeling job in Amazon SageMaker GroundTruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52319462-e444-4c17-8c55-ae334cf9763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81a053-6502-453a-805a-5e640df0733f",
   "metadata": {},
   "source": [
    "## 2. Download images and labels from the labeling job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5404285-dd6a-451e-99e1-da36e64d14d8",
   "metadata": {},
   "source": [
    "#### First we have to download the annotation manifest generated by Amazon SageMaker GroundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eaa97fd-470e-4e81-abc5-0880f174ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_job_name = \"OB-Test-1\"\n",
    "gt_output_manifest_bucket = \"buzecd-aiml-demos\" # name of the bucket\n",
    "gt_output_manifest_file = \"obb-uc2/training-images/OB-Test-1/manifests/output/output.manifest\" # include prefix in the path\n",
    "labels = [\"stop\", \"pedestrian\"]\n",
    "s3.meta.client.download_file(gt_output_manifest_bucket, gt_output_manifest_file, 'gt_manifest.txt') # download the manifest to your local environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542354a0-0fc6-43a0-b662-1cc76d260857",
   "metadata": {},
   "source": [
    "#### Next, we are going to split our images into two sets, training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe944338-9dd7-46ed-bf03-fb3be695a3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The manifest contains 211 annotations.\n",
      "168 will be used for training.\n",
      "43 will be used for validation.\n"
     ]
    }
   ],
   "source": [
    "with open('gt_manifest.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    data = numpy.array(lines)\n",
    "    train_data, validation_data = train_test_split(data, test_size=0.2)\n",
    "print(\"The manifest contains {} annotations.\".format(len(data)))\n",
    "print(\"{} will be used for training.\".format(len(train_data)))\n",
    "print(\"{} will be used for validation.\".format(len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ddeec-0717-482e-b063-3446861f69dc",
   "metadata": {},
   "source": [
    "#### Now we have our 2 datasets, lets download the images and create the annotation files in YOLO friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273e2488-8195-445a-b42e-c26cdb2a24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_to_yolo(gt_manifest_data, dataset_category):\n",
    "    print(\"Downloading images and labels for the {} dataset\".format(dataset_category))\n",
    "    for line in gt_manifest_data:\n",
    "        line = json.loads(line)\n",
    "        uri = line[\"source-ref\"]\n",
    "        file_bucket = uri.split(\"/\")[2]\n",
    "        file_image_name = uri.split(\"/\")[-1]\n",
    "        file_txt_name = '.'.join(file_image_name.split(\".\")[:-1]) + \".txt\"\n",
    "        file_txt_path = \"training_data/labels/{}/{}\".format(dataset_category, file_txt_name)\n",
    "        file_path = '/'.join(uri.split(\"/\")[3:])\n",
    "        # Download image\n",
    "        s3.meta.client.download_file(file_bucket, file_path, \"training_data/images/{}/{}\".format(dataset_category,file_image_name))\n",
    "        # Create txt with annotations\n",
    "        with open(file_txt_path, 'w') as target:\n",
    "            for annotation in line[gt_job_name][\"annotations\"]:\n",
    "                class_id = annotation[\"class_id\"]\n",
    "                center_x = (annotation[\"left\"] + (annotation[\"width\"]/2)) / line[gt_job_name][\"image_size\"][0][\"width\"]\n",
    "                center_y = (annotation[\"top\"] + (annotation[\"height\"]/2)) / line[gt_job_name][\"image_size\"][0][\"height\"]\n",
    "                w = annotation[\"width\"] / line[gt_job_name][\"image_size\"][0][\"width\"]\n",
    "                h = annotation[\"height\"] / line[gt_job_name][\"image_size\"][0][\"height\"]\n",
    "                data = \"{} {} {} {} {}\\n\".format(class_id, center_x, center_y, w, h)\n",
    "                target.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2462f2f-3dc0-4c02-ad53-4db030078ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images and labels for the train dataset\n",
      "Downloading images and labels for the validation dataset\n"
     ]
    }
   ],
   "source": [
    "ground_truth_to_yolo(train_data, \"train\")\n",
    "ground_truth_to_yolo(validation_data, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd94f5-f78a-44a7-8c7d-a23cd3cfe90e",
   "metadata": {},
   "source": [
    "#### Lets make sure there are the same number of elements in our directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44534876-38eb-4c01-a0ed-1447911ae23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 168 elements in training_data/images/train\n",
      "There are 168 elements in training_data/labels/train\n",
      "There are 43 elements in training_data/images/validation\n",
      "There are 43 elements in training_data/labels/validation\n"
     ]
    }
   ],
   "source": [
    "def count_files(dirs):\n",
    "    for directory in dirs:\n",
    "        number = len([1 for x in list(os.scandir(directory)) if x.is_file()])\n",
    "        print(\"There are {} elements in {}\".format(number, directory))\n",
    "\n",
    "\n",
    "count_files(dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388275f-d930-44ac-be59-cd9e1312c7a4",
   "metadata": {},
   "source": [
    "####Â Now let's add these data sources to the data library in the yolov5 folder for our model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03755c69-4523-4d50-9cb2-e6a0b4ea2820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ../training_data\n",
      "\n",
      "train: images/train\n",
      "\n",
      "val: images/validation\n",
      "\n",
      "names:\n",
      "\n",
      "  0: stop\n",
      "\n",
      "  1: pedestrian\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"yolov5/data/custom-model.yaml\", 'w') as target:\n",
    "    target.write(\"path: ../training_data\\n\")\n",
    "    target.write(\"train: images/train\\n\")\n",
    "    target.write(\"val: images/validation\\n\")\n",
    "    target.write(\"names:\\n\")\n",
    "    for i, label in enumerate(labels):\n",
    "        target.write(\"  {}: {}\\n\".format(i, label))\n",
    "        \n",
    "with open('yolov5/data/custom-model.yaml') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b8fbd-ccdb-4440-af14-ea428bcc1cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Train the custom YOLOv5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2f8907-0640-4aa0-a4b0-80b061b43659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=yolov5/data/custom-model.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=4, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "fatal: cannot change to '/root/Object': No such file or directory\n",
      "YOLOv5 ðŸš€ 2022-9-21 Python-3.8.10 torch-1.10.2+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs in Weights & Biases\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 156MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "[2022-09-21 12:27:39.789 pytorch-1-10-gpu-py-ml-g4dn-xlarge-5086b554a12da40ba14f4b244605:2282 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[2022-09-21 12:27:39.914 pytorch-1-10-gpu-py-ml-g4dn-xlarge-5086b554a12da40ba14f4b244605:2282 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/tra\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/training_data/labels/train.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:01<00:00, 154.20\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/train\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/training_data/labels/validation.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:00<00:00, 109.81it/s\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.60 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to yolov5/runs/train/exp/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1myolov5/runs/train/exp\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49      3.72G     0.1329    0.02596    0.02979         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49      3.72G     0.1227    0.02214      0.027         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47   0.000295     0.0827   0.000171   4.51e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49      3.72G     0.1165    0.02012    0.02404         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47   0.000513       0.16    0.00112   0.000265\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49      3.72G     0.1095    0.01884    0.02122         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47   0.000642      0.185    0.00102    0.00021\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49      3.72G     0.0965    0.02033    0.01755         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47     0.0011      0.345    0.00839    0.00253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49      3.72G    0.09374    0.02104    0.01621         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47    0.00202      0.547      0.102     0.0263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49      3.72G    0.08753    0.01921    0.01448         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.844        0.1      0.204     0.0673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49      3.72G    0.08315    0.01941    0.01265         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.564     0.0616     0.0689     0.0167\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49      3.72G     0.0794    0.01802   0.009836          9        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.696       0.16       0.15     0.0338\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49      3.72G    0.07949    0.01797   0.008903          9        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.688       0.24      0.261     0.0624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49      3.72G    0.07306    0.01842   0.007466         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47       0.59        0.1     0.0787     0.0169\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49      3.72G    0.06722    0.01784   0.007812         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.109      0.323      0.109     0.0304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49      3.72G    0.06511    0.01733    0.00593         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.365      0.451      0.276     0.0808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49      3.72G    0.06511    0.01609   0.005014         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.308      0.599      0.296      0.131\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49      3.72G    0.06324    0.01635   0.006464         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.476      0.579      0.462      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49      3.72G    0.05938    0.01643   0.004782         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.423      0.516      0.457      0.167\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49      3.72G    0.05558    0.01574   0.005147         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.528      0.613      0.599      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49      3.72G    0.05381    0.01447   0.004641         10        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.505      0.758      0.602      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49      3.72G    0.05283     0.0132   0.003642         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.422      0.826      0.562      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49      3.72G    0.05073    0.01356   0.004058         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.509      0.917      0.601      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49      3.72G    0.04652    0.01378   0.004251         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.509      0.849      0.541      0.202\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49      3.72G    0.04955    0.01251   0.002461         11        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.563      0.826      0.676      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49      3.72G    0.04577    0.01249   0.004174         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.599      0.806      0.674       0.28\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49      3.72G    0.04703    0.01146    0.00287          9        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.808      0.833      0.875      0.371\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49      3.72G    0.04371    0.01168   0.003046         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.836      0.896      0.901      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49      3.72G    0.04126    0.01109    0.00282         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.756       0.87      0.833      0.362\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49      3.72G    0.04388    0.01091   0.003554         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.759      0.957      0.893      0.434\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49      3.72G    0.04201     0.0108   0.003155         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.745      0.895      0.891      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49      3.72G    0.04198    0.01029   0.002355         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.736      0.845      0.819      0.346\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49      3.72G    0.03978     0.0103   0.003245         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.742      0.893      0.835      0.411\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/49      3.72G    0.04139   0.009128   0.002547         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.837      0.873      0.916      0.425\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49      3.72G    0.03983     0.0101   0.002905         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.868      0.867      0.943       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49      3.72G    0.03828   0.009772   0.002302         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47       0.75      0.909      0.876      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49      3.72G    0.03847   0.009418   0.001836         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.803       0.96      0.891      0.394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/49      3.72G    0.03716   0.009342   0.002033         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.814       0.98      0.927      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49      3.72G    0.03684   0.009421   0.001517         11        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.838       0.96       0.92      0.473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49      3.72G    0.03636   0.009038   0.002275         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.838      0.962      0.913      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49      3.72G    0.03427   0.008775   0.002189         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.831       0.98      0.914      0.461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49      3.72G    0.03245   0.009532   0.001819         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.843       0.96      0.931      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49      3.72G    0.03239   0.008897   0.002444         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.839       0.98      0.931      0.505\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49      3.72G     0.0308   0.008682    0.00199         11        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.822       0.98      0.927      0.483\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49      3.72G     0.0318   0.008976   0.002583         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.831       0.98      0.929      0.502\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49      3.72G    0.03085   0.008323   0.001689         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.831       0.98       0.92       0.49\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49      3.72G    0.03066   0.009334   0.001621         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47       0.83       0.98       0.92      0.462\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49      3.72G    0.03027   0.008285   0.001429         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47       0.83       0.98      0.923      0.488\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49      3.72G    0.03077      0.009   0.002069         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.828       0.98      0.921      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49      3.72G    0.02886   0.008413   0.001627         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.829       0.96      0.926       0.52\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49      3.72G    0.02922   0.008304   0.002412         14        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.827      0.969      0.931      0.519\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49      3.72G    0.02847   0.008123   0.001913         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.828       0.96      0.931      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49      3.72G    0.02806   0.008324   0.001336         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.827      0.961      0.935      0.526\n",
      "\n",
      "50 epochs completed in 0.045 hours.\n",
      "Optimizer stripped from yolov5/runs/train/exp/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from yolov5/runs/train/exp/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating yolov5/runs/train/exp/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         43         47      0.829       0.96      0.931      0.527\n",
      "                  stop         43         25      0.771       0.92      0.899      0.509\n",
      "            pedestrian         43         22      0.886          1      0.962      0.545\n",
      "Results saved to \u001b[1myolov5/runs/train/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/train.py --workers 4 --device 0 --img 640 --batch 16 --epochs 50 --data yolov5/data/custom-model.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5835f-e975-408f-97f1-3c47f44ba56e",
   "metadata": {},
   "source": [
    "## 4. Make inferences with the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3dfee9-e2fc-4058-a5aa-2ffe186d4a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/runs/train/exp/weights/best.pt'], source=street3.mp4, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "fatal: cannot change to '/root/Object': No such file or directory\n",
      "YOLOv5 ðŸš€ 2022-9-21 Python-3.8.10 torch-1.10.2+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "Fusing layers... \n",
      "[2022-09-21 12:30:48.268 pytorch-1-10-gpu-py-ml-g4dn-xlarge-5086b554a12da40ba14f4b244605:2425 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[2022-09-21 12:30:48.403 pytorch-1-10-gpu-py-ml-g4dn-xlarge-5086b554a12da40ba14f4b244605:2425 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "video 1/1 (1/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 24.8ms\n",
      "video 1/1 (2/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.9ms\n",
      "video 1/1 (3/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (4/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (5/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (6/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (7/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.7ms\n",
      "video 1/1 (8/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (9/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (10/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (11/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (12/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.6ms\n",
      "video 1/1 (13/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (14/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (15/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.0ms\n",
      "video 1/1 (16/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.6ms\n",
      "video 1/1 (17/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (18/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.2ms\n",
      "video 1/1 (19/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (20/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (21/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (22/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.6ms\n",
      "video 1/1 (23/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (24/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (25/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (26/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (27/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (28/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (29/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (30/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.5ms\n",
      "video 1/1 (31/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (32/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.2ms\n",
      "video 1/1 (33/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (34/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (35/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (36/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (37/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (38/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (39/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (40/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (41/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (42/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.5ms\n",
      "video 1/1 (43/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 21.2ms\n",
      "video 1/1 (44/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (45/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.8ms\n",
      "video 1/1 (46/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (47/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (48/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (49/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.7ms\n",
      "video 1/1 (50/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (51/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (52/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.0ms\n",
      "video 1/1 (53/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.8ms\n",
      "video 1/1 (54/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (55/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (56/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (57/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (58/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (59/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (60/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (61/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (62/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (63/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (64/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (65/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (66/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.8ms\n",
      "video 1/1 (67/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (68/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (69/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.8ms\n",
      "video 1/1 (70/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (71/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (72/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (73/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (74/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (75/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.4ms\n",
      "video 1/1 (76/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.0ms\n",
      "video 1/1 (77/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.2ms\n",
      "video 1/1 (78/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (79/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.6ms\n",
      "video 1/1 (80/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (81/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (82/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (83/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (84/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (85/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.0ms\n",
      "video 1/1 (86/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (87/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (88/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (89/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (90/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (91/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (92/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (93/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (94/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (95/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (96/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (97/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (98/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.5ms\n",
      "video 1/1 (99/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (100/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (101/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (102/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (103/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (104/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (105/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.2ms\n",
      "video 1/1 (106/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (107/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (108/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (109/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (110/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (111/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (112/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.2ms\n",
      "video 1/1 (113/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.0ms\n",
      "video 1/1 (114/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (115/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (116/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (117/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (118/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (119/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.4ms\n",
      "video 1/1 (120/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (121/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (122/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.4ms\n",
      "video 1/1 (123/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.8ms\n",
      "video 1/1 (124/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.4ms\n",
      "video 1/1 (125/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (126/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.3ms\n",
      "video 1/1 (127/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.8ms\n",
      "video 1/1 (128/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.0ms\n",
      "video 1/1 (129/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.8ms\n",
      "video 1/1 (130/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (131/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (132/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (133/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (134/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (135/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.5ms\n",
      "video 1/1 (136/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 20.5ms\n",
      "video 1/1 (137/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (138/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.0ms\n",
      "video 1/1 (139/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (140/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.4ms\n",
      "video 1/1 (141/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.3ms\n",
      "video 1/1 (142/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.1ms\n",
      "video 1/1 (143/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (144/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.8ms\n",
      "video 1/1 (145/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (146/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (147/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.5ms\n",
      "video 1/1 (148/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (149/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (150/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (151/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.4ms\n",
      "video 1/1 (152/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.4ms\n",
      "video 1/1 (153/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (154/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.2ms\n",
      "video 1/1 (155/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 19.0ms\n",
      "video 1/1 (156/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (157/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (158/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.8ms\n",
      "video 1/1 (159/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.6ms\n",
      "video 1/1 (160/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (161/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (162/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (163/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (164/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.9ms\n",
      "video 1/1 (165/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (166/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (167/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (168/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (169/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.2ms\n",
      "video 1/1 (170/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (171/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.9ms\n",
      "video 1/1 (172/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (173/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (174/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (175/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.4ms\n",
      "video 1/1 (176/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (177/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (178/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (179/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (180/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (181/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (182/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (183/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.0ms\n",
      "video 1/1 (184/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (185/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (186/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.1ms\n",
      "video 1/1 (187/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (188/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.2ms\n",
      "video 1/1 (189/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (190/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (191/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (192/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.9ms\n",
      "video 1/1 (193/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (194/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (195/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (196/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 15.7ms\n",
      "video 1/1 (197/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (198/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (199/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.8ms\n",
      "video 1/1 (200/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (201/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (202/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (203/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (204/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.6ms\n",
      "video 1/1 (205/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (206/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.3ms\n",
      "video 1/1 (207/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (208/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.6ms\n",
      "video 1/1 (209/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (210/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (211/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (212/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.5ms\n",
      "video 1/1 (213/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.3ms\n",
      "video 1/1 (214/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (215/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.5ms\n",
      "video 1/1 (216/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (217/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (218/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.0ms\n",
      "video 1/1 (219/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.3ms\n",
      "video 1/1 (220/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.7ms\n",
      "video 1/1 (221/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.0ms\n",
      "video 1/1 (222/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.0ms\n",
      "video 1/1 (223/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.0ms\n",
      "video 1/1 (224/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.2ms\n",
      "video 1/1 (225/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.9ms\n",
      "video 1/1 (226/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.6ms\n",
      "video 1/1 (227/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.4ms\n",
      "video 1/1 (228/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 18.4ms\n",
      "video 1/1 (229/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.8ms\n",
      "video 1/1 (230/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 18.2ms\n",
      "video 1/1 (231/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.6ms\n",
      "video 1/1 (232/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (233/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.9ms\n",
      "video 1/1 (234/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.4ms\n",
      "video 1/1 (235/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.2ms\n",
      "video 1/1 (236/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 18.2ms\n",
      "video 1/1 (237/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 18.4ms\n",
      "video 1/1 (238/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 18.3ms\n",
      "video 1/1 (239/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.1ms\n",
      "video 1/1 (240/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.1ms\n",
      "video 1/1 (241/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.6ms\n",
      "video 1/1 (242/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.1ms\n",
      "video 1/1 (243/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.5ms\n",
      "video 1/1 (244/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (245/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.2ms\n",
      "video 1/1 (246/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.4ms\n",
      "video 1/1 (247/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.9ms\n",
      "video 1/1 (248/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.1ms\n",
      "video 1/1 (249/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.8ms\n",
      "video 1/1 (250/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.1ms\n",
      "video 1/1 (251/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 22.0ms\n",
      "video 1/1 (252/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.5ms\n",
      "video 1/1 (253/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.9ms\n",
      "video 1/1 (254/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.1ms\n",
      "video 1/1 (255/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.9ms\n",
      "video 1/1 (256/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (257/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 20.4ms\n",
      "video 1/1 (258/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.0ms\n",
      "video 1/1 (259/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.7ms\n",
      "video 1/1 (260/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.9ms\n",
      "video 1/1 (261/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.1ms\n",
      "video 1/1 (262/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.7ms\n",
      "video 1/1 (263/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.2ms\n",
      "video 1/1 (264/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.9ms\n",
      "video 1/1 (265/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.1ms\n",
      "video 1/1 (266/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.8ms\n",
      "video 1/1 (267/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.4ms\n",
      "video 1/1 (268/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.4ms\n",
      "video 1/1 (269/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.8ms\n",
      "video 1/1 (270/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.6ms\n",
      "video 1/1 (271/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (272/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.7ms\n",
      "video 1/1 (273/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.8ms\n",
      "video 1/1 (274/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.1ms\n",
      "video 1/1 (275/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.9ms\n",
      "video 1/1 (276/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.3ms\n",
      "video 1/1 (277/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.5ms\n",
      "video 1/1 (278/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.9ms\n",
      "video 1/1 (279/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.5ms\n",
      "video 1/1 (280/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (281/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (282/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (283/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (284/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.0ms\n",
      "video 1/1 (285/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.8ms\n",
      "video 1/1 (286/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.0ms\n",
      "video 1/1 (287/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.8ms\n",
      "video 1/1 (288/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.3ms\n",
      "video 1/1 (289/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.0ms\n",
      "video 1/1 (290/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.7ms\n",
      "video 1/1 (291/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.7ms\n",
      "video 1/1 (292/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (293/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.0ms\n",
      "video 1/1 (294/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.3ms\n",
      "video 1/1 (295/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.9ms\n",
      "video 1/1 (296/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (297/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.6ms\n",
      "video 1/1 (298/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.2ms\n",
      "video 1/1 (299/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.7ms\n",
      "video 1/1 (300/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.1ms\n",
      "video 1/1 (301/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.8ms\n",
      "video 1/1 (302/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.7ms\n",
      "video 1/1 (303/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.1ms\n",
      "video 1/1 (304/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.0ms\n",
      "video 1/1 (305/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.6ms\n",
      "video 1/1 (306/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (307/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.7ms\n",
      "video 1/1 (308/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.6ms\n",
      "video 1/1 (309/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.2ms\n",
      "video 1/1 (310/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.1ms\n",
      "video 1/1 (311/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.5ms\n",
      "video 1/1 (312/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.2ms\n",
      "video 1/1 (313/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.9ms\n",
      "video 1/1 (314/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.2ms\n",
      "video 1/1 (315/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.6ms\n",
      "video 1/1 (316/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.2ms\n",
      "video 1/1 (317/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.6ms\n",
      "video 1/1 (318/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.6ms\n",
      "video 1/1 (319/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.5ms\n",
      "video 1/1 (320/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.2ms\n",
      "video 1/1 (321/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.6ms\n",
      "video 1/1 (322/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.1ms\n",
      "video 1/1 (323/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.0ms\n",
      "video 1/1 (324/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.9ms\n",
      "video 1/1 (325/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.9ms\n",
      "video 1/1 (326/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.6ms\n",
      "video 1/1 (327/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.1ms\n",
      "video 1/1 (328/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.4ms\n",
      "video 1/1 (329/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.8ms\n",
      "video 1/1 (330/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.4ms\n",
      "video 1/1 (331/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.0ms\n",
      "video 1/1 (332/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.3ms\n",
      "video 1/1 (333/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.9ms\n",
      "video 1/1 (334/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 16.7ms\n",
      "video 1/1 (335/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.4ms\n",
      "video 1/1 (336/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 17.0ms\n",
      "video 1/1 (337/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (338/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.3ms\n",
      "video 1/1 (339/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.3ms\n",
      "video 1/1 (340/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (341/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.1ms\n",
      "video 1/1 (342/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.5ms\n",
      "video 1/1 (343/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (344/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.6ms\n",
      "video 1/1 (345/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.6ms\n",
      "video 1/1 (346/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.3ms\n",
      "video 1/1 (347/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.0ms\n",
      "video 1/1 (348/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (349/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 19.6ms\n",
      "video 1/1 (350/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.4ms\n",
      "video 1/1 (351/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (352/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (353/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.9ms\n",
      "video 1/1 (354/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.7ms\n",
      "video 1/1 (355/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (356/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.1ms\n",
      "video 1/1 (357/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.2ms\n",
      "video 1/1 (358/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.5ms\n",
      "video 1/1 (359/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (360/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.3ms\n",
      "video 1/1 (361/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.1ms\n",
      "video 1/1 (362/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.6ms\n",
      "video 1/1 (363/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.3ms\n",
      "video 1/1 (364/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.9ms\n",
      "video 1/1 (365/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.6ms\n",
      "video 1/1 (366/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (367/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.5ms\n",
      "video 1/1 (368/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.2ms\n",
      "video 1/1 (369/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (370/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.2ms\n",
      "video 1/1 (371/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.5ms\n",
      "video 1/1 (372/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (373/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (374/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.7ms\n",
      "video 1/1 (375/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.7ms\n",
      "video 1/1 (376/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.1ms\n",
      "video 1/1 (377/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.8ms\n",
      "video 1/1 (378/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.3ms\n",
      "video 1/1 (379/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 16.8ms\n",
      "video 1/1 (380/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 20.9ms\n",
      "video 1/1 (381/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.0ms\n",
      "video 1/1 (382/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.5ms\n",
      "video 1/1 (383/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.7ms\n",
      "video 1/1 (384/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.8ms\n",
      "video 1/1 (385/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.6ms\n",
      "video 1/1 (386/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.6ms\n",
      "video 1/1 (387/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.7ms\n",
      "video 1/1 (388/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (389/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (390/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.2ms\n",
      "video 1/1 (391/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.4ms\n",
      "video 1/1 (392/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.6ms\n",
      "video 1/1 (393/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.0ms\n",
      "video 1/1 (394/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.9ms\n",
      "video 1/1 (395/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.8ms\n",
      "video 1/1 (396/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.4ms\n",
      "video 1/1 (397/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.3ms\n",
      "video 1/1 (398/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.4ms\n",
      "video 1/1 (399/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.8ms\n",
      "video 1/1 (400/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 18.5ms\n",
      "video 1/1 (401/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.6ms\n",
      "video 1/1 (402/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.3ms\n",
      "video 1/1 (403/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 19.0ms\n",
      "video 1/1 (404/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 18.4ms\n",
      "video 1/1 (405/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.1ms\n",
      "video 1/1 (406/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.8ms\n",
      "video 1/1 (407/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.5ms\n",
      "video 1/1 (408/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.9ms\n",
      "video 1/1 (409/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 3 stops, 18.1ms\n",
      "video 1/1 (410/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.4ms\n",
      "video 1/1 (411/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 17.0ms\n",
      "video 1/1 (412/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 2 stops, 18.9ms\n",
      "video 1/1 (413/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.0ms\n",
      "video 1/1 (414/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.9ms\n",
      "video 1/1 (415/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.4ms\n",
      "video 1/1 (416/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.6ms\n",
      "video 1/1 (417/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.1ms\n",
      "video 1/1 (418/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.5ms\n",
      "video 1/1 (419/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.0ms\n",
      "video 1/1 (420/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 16.9ms\n",
      "video 1/1 (421/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.0ms\n",
      "video 1/1 (422/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 20.4ms\n",
      "video 1/1 (423/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 1 stop, 17.5ms\n",
      "video 1/1 (424/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 18.7ms\n",
      "video 1/1 (425/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (426/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.4ms\n",
      "video 1/1 (427/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.6ms\n",
      "video 1/1 (428/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (429/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (430/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (431/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.7ms\n",
      "video 1/1 (432/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (433/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.3ms\n",
      "video 1/1 (434/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (435/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 15.8ms\n",
      "video 1/1 (436/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.8ms\n",
      "video 1/1 (437/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.6ms\n",
      "video 1/1 (438/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.2ms\n",
      "video 1/1 (439/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (440/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.3ms\n",
      "video 1/1 (441/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.7ms\n",
      "video 1/1 (442/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.7ms\n",
      "video 1/1 (443/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.9ms\n",
      "video 1/1 (444/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 19.2ms\n",
      "video 1/1 (445/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (446/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.7ms\n",
      "video 1/1 (447/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "video 1/1 (448/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 17.5ms\n",
      "video 1/1 (449/449) /root/Object Detection at AWS/YOLOv5/1 - SM Studio Training/street3.mp4: 384x640 (no detections), 16.1ms\n",
      "Speed: 0.5ms pre-process, 17.2ms inference, 0.5ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov5/runs/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/detect.py --weights yolov5/runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source street3.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4bddd2-faa8-4ff4-8713-a9a65e5666d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
